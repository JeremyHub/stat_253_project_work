```{r hw2_setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, eval=TRUE)
```

# Homework 2 {-}

Jeremy Hubinger, and Tamur Asar


<br><br><br>




## Project Work {-}

### Instructions {-} 

**Goal:** Begin an analysis of your dataset to answer your **regression** research question.

<br>

**Collaboration:** Form a team (2-3 members) for the project and this part can be done as a team. Only one team member should submit a Project Work section. Make sure you include the full names of all of the members in your write up. 

<br>

**Data cleaning:** If your dataset requires any cleaning (e.g., merging datasets, creation of new variables), first consult the [R Resources page](r-resources.html) to see if your questions are answered there. If not, post on the #rcode-questions channel in our Slack workspace to ask for help. *Please ask for help early and regularly* to avoid stressful workloads.

<br>

### Required Analyses {-}

1. **Initial investigation: ignoring nonlinearity (for now)**
    a. Use ordinary least squares (OLS) by using the `lm` engine and LASSO (`glmnet` engine) to build  a series of initial regression models for your quantitative outcome as a function of the predictors of interest. (As part of data cleaning, exclude any variables that you don't want to consider as predictors.)
        - You'll need two model specifications, `lm_spec` and `lm_lasso_spec` (you'll need to tune this one).
    b. For each set of variables, you'll need a `recipe` with the `formula`, `data`, and pre-processing steps
        - You may want to have steps in your recipe that remove variables with near zero variance (`step_nzv()`), remove variables that are highly correlated with other variables (`step_corr()`), normalize all quantitative predictors (`step_normalize(all_numeric_predictors())`) and add indicator variables for any categorical variables (`step_dummy(all_nominal_predictors())`).
        - These models should not include any transformations to deal with nonlinearity. You'll explore this in the next investigation.
    c. Estimate the test performance of the models using CV. Report and interpret (with units) the CV metric estimates along with a measure of uncertainty in the estimate (`std_error` is readily available when you used `collect_metrics(summarize=TRUE)`).
        - Compare estimated test performance across the models. Which models(s) might you prefer?
    d. Use residual plots to evaluate whether some quantitative predictors might be better modeled with nonlinear relationships.
    e. Which variables do you think are the most important predictors of your quantitative outcome? Justify your answer. Do the methods you've applied reach consensus on which variables are most important? What insights are expected? Surprising?
        - Note that if some (but not all) of the indicator terms for a categorical predictor are selected in the final models, the whole predictor should be treated as selected.

<br>

#### Your Work {-}

a & b.

```{r}
# library statements 
# read in data
library(readr)
library(ggplot2)
library(dplyr)
library(tidymodels)
tidymodels_prefer()

fires <- read_csv("forestfires.csv")
```

The dataset has a variety of variables that might impact the strength and likelihood of fires. These variables include mainly things about weather, like temperature, pressure, wind, rain, ect. The variable we are trying to predict is "area" which is the area burned by fires on that day.

```{r}
# data cleaning
fires <- fires %>%
    # we get rid of x and y because those are not relevant to the predictino of fire (they are the coordinates of the fires within the dataset)
    # we get rid of day and month because there isnt a huge reason that they should impact the fires, we are more interested in how weather factors impact fires, not day of the week or month
    select(-X, -Y, -day, -month)
```

```{r}
# creation of cv folds
set.seed(88)
fires_cv10 <- vfold_cv(fires, v = 10)
```

We now fit multiple types of models. One model is fit using simple LM regression with all predictors. The second model is fit using LASSO regression to attempt to provide some amount of filtering to the model predictors. Both models are fit and compared later on.

```{r}
# model spec
lm_spec <-
    linear_reg() %>% 
    set_engine(engine = 'lm') %>% 
    set_mode('regression')

# this is the LASSO model
lm_lasso_spec <- 
  linear_reg() %>%
  set_args(mixture = 1, penalty = tune()) %>%
  set_engine(engine = 'glmnet') %>%
  set_mode('regression')
```

```{r}
# recipes & workflows
# both models are fit with the same recipe because they both start with all predictors and all predictors are normalized
full_rec <- recipe(area ~ ., data = fires) %>%
    step_nzv(all_predictors()) %>% # removes variables with the same value
    step_normalize(all_numeric_predictors()) %>% # important standardization step for LASSO
    step_dummy(all_nominal_predictors())  # creates indicator variables for categorical variables

full_lm_wf <- workflow() %>%
    add_recipe(full_rec) %>%
    add_model(lm_spec)

lasso_wf_tune <- workflow() %>% 
  add_recipe(full_rec) %>%
  add_model(lm_lasso_spec) 
```

```{r}
# fit & tune models
full_lm_wf <- workflow() %>%
    add_recipe(full_rec) %>%
    add_model(lm_spec)
    
full_model <- fit(full_lm_wf, data = fires) 
```

```{r}
# Tune Model (trying a variety of values of Lambda penalty)
penalty_grid <- grid_regular(
  penalty(range = c(-3, 0.5)), #log10 transformed 
  levels = 30)

tune_output <- tune_grid( # new function for tuning parameters
  lasso_wf_tune, # workflow
  resamples = fires_cv10, # cv folds
  metrics = metric_set(rmse, mae),
  grid = penalty_grid # penalty grid defined above
)
```

```{r}
 autoplot(tune_output) + theme_classic()

 collect_metrics(tune_output) %>%
   filter(.metric == 'rmse') %>% # or choose mae
   select(penalty, rmse = mean)

 best_penalty <- select_best(tune_output, metric = 'rmse')

best_penalty

# the best penalty within the range is 3.162278

final_wf <- finalize_workflow(lasso_wf_tune, best_penalty)

final_fit <- fit(final_wf, data = fires)

tidy(final_fit)
```
It seems as though the only things that matter in the model aer DMC, RH, and Temp. Temp being the main factor, as that coefficient is about 5x larger than the other two.

c.

```{r}
#  calculate/collect CV metrics

full_model %>% tidy()

best_penalty <- select_by_one_std_err(tune_output, metric = 'rmse', desc(penalty))

final_wf <- finalize_workflow(lasso_wf_tune, best_penalty)

final_fit <- fit(final_wf, data = fires)

tidy(final_fit)
```
Based on the above information, using the LASSO model to tell what variables are the most important, we can see that temperature is the most important variable and that most others are at or close to zero. This is much more readable than the LM model which has a lot of importance on a lot of different variables which don't seem to matter, and actually hurt the model's performance on test data.

```{r}
LASSO_model_cv <- fit_resamples(final_fit,
  resamples = fires_cv10, 
  metrics = metric_set(rmse, rsq, mae)
)

LASSO_model_cv %>% collect_metrics(summarize=TRUE)

LM_model_cv <- fit_resamples(full_model,
  resamples = fires_cv10, 
  metrics = metric_set(rmse, rsq, mae)
)

LM_model_cv %>% collect_metrics(summarize=TRUE)
```
As we can see in the metrics above, the normal LM model with all the variables performs worse than the LASSO model which is much simpler. Because of the better performance (likely due to overfitting in the LM model's case) and simpler model coefficients we prefer the LASSO model.
 
d.

```{r}
# visual residuals
final_fit %>% tidy() %>% filter(estimate != 0)

lasso_mod_out <- final_fit %>%
    predict(new_data = fires) %>%
    bind_cols(fires) %>%
    mutate(resid = area - .pred)

lm_mod_out <- full_model %>%
    predict(new_data = fires) %>%
    bind_cols(fires) %>%
    mutate(resid = area - .pred)

ggplot(lm_mod_out, aes(x = temp, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic() +
    ylim(-50,150) +
    ggtitle("LM model residual plot")

ggplot(lasso_mod_out, aes(x = temp, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic() +
    ylim(-50,150) +
    ggtitle("LM model residual plot")

ggplot(lm_mod_out, aes(x = temp, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic() +
    ggtitle("LASSO model residual plot")

ggplot(lasso_mod_out, aes(x = temp, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic() +
    ggtitle("LASSO model residual plot")

```
The residual plots above confirm that the LASSO model is a better predictor. As we can see, the resuiduals for the lASSO model are tighter to the y axis when comapred to the LM model.

e.

As mentioned earlier, the variable with the most imporatnce seems to be temperature, which makes sense. We can see this by looking at the coeffecicents of the better LASSO model. There are some other predictors, like a lot of fires seemed to happen in september, but those other predictors are much less impactful on the model's predition when compared to temperature.

<br>  

Part 2:

```{r}
# Natural Spline Recipe
ns2_rec <- full_rec %>%
  step_ns(temp, deg_free = 3) %>%
  step_ns(RH, deg_free = 3) %>%
  step_ns(DMC, deg_free = 3) %>%
  step_ns(FFMC, deg_free = 3) %>%
  step_ns(DC, deg_free = 3) %>%
  step_ns(ISI, deg_free = 3) %>%
  step_ns(wind, deg_free = 3) # natural cubic spline (higher deg_free means more knots)

# Workflow (Recipe + Model)
wf <- workflow() %>%
    add_recipe(ns2_rec) %>%
    add_model(lm_spec)

# CV to Evaluate
cv_output <- fit_resamples(
  wf, # workflow
  resamples = fires_cv10, # cv folds
  metrics = metric_set(rsq, mae, rmse)
)
```

```{r}
# Fit with all data
ns_mod <- fit(
  wf, #workflow
  data = fires
)

ns_mod_output <- fires %>%
  bind_cols(predict(ns_mod, new_data = fires)) %>%
    mutate(resid = area - .pred)
```

```{r}
cv_output %>% collect_metrics(summarize = TRUE)
LM_model_cv %>% collect_metrics(summarize=TRUE)
LASSO_model_cv %>% collect_metrics(summarize=TRUE)
```


<br>

2. **Summarize investigations**
    - Decide on an overall best model based on your investigations so far. To do this, make clear your analysis goals. Predictive accuracy? Interpretability? A combination of both?
  A mix of interpretability and predictive accuracy is best. The interpradability of our model is important because if someone actually wants to predict a fire using our model, they should only have to take and interpret the measurements that matter the most. The predictive power of the model obviously should not be significantly sacrificed for this goal because predicting fires is the outcome that matters. Taking both of these factors into account, we believe that the LASSO model is far superior to the LM model, because the LASSO model only has 3 relevant predictors and its residual bound is much tighter, especially within the negative residuals, which means that in the LM model there were significantly more false positives. When false positives did occur, the LASSO model predicted much smaller fires than the LM model. However, both models were not very good at predicting large fires.


<br>

3. **Societal impact**
    - Are there any harms that may come from your analyses and/or how the data were collected?
    - What cautions do you want to keep in mind when communicating your work?
  If these models are taken to be accurate, there could be real harms that occur. Specifically, that our models do not predict big fires very well at all. That is the take away that should be taken from this analysis, that big fires are very unpredictable.
  Another possible harm is if the area variable is interpreted as pure area, without consideration for the transformation that occurred prior to analysis. The area was transformed with an ln(x+1) function. Therefore, all predictions should be interpreted through the inverse of this transform to be interpreted in any meaningful way.
    
    
    
    
    
    
    




