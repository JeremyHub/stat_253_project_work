```{r hw2_setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, eval=FALSE)
```

# Homework 2 {-}

Jeremy Hubinger, and Tamur Asar


<br><br><br>




## Project Work {-}

### Instructions {-} 

**Goal:** Begin an analysis of your dataset to answer your **regression** research question.

<br>

**Collaboration:** Form a team (2-3 members) for the project and this part can be done as a team. Only one team member should submit a Project Work section. Make sure you include the full names of all of the members in your write up. 

<br>

**Data cleaning:** If your dataset requires any cleaning (e.g., merging datasets, creation of new variables), first consult the [R Resources page](r-resources.html) to see if your questions are answered there. If not, post on the #rcode-questions channel in our Slack workspace to ask for help. *Please ask for help early and regularly* to avoid stressful workloads.

<br>

### Required Analyses {-}

1. **Initial investigation: ignoring nonlinearity (for now)**
    a. Use ordinary least squares (OLS) by using the `lm` engine and LASSO (`glmnet` engine) to build  a series of initial regression models for your quantitative outcome as a function of the predictors of interest. (As part of data cleaning, exclude any variables that you don't want to consider as predictors.)
        - You'll need two model specifications, `lm_spec` and `lm_lasso_spec` (you'll need to tune this one).
    b. For each set of variables, you'll need a `recipe` with the `formula`, `data`, and pre-processing steps
        - You may want to have steps in your recipe that remove variables with near zero variance (`step_nzv()`), remove variables that are highly correlated with other variables (`step_corr()`), normalize all quantitative predictors (`step_normalize(all_numeric_predictors())`) and add indicator variables for any categorical variables (`step_dummy(all_nominal_predictors())`).
        - These models should not include any transformations to deal with nonlinearity. You'll explore this in the next investigation.
    c. Estimate the test performance of the models using CV. Report and interpret (with units) the CV metric estimates along with a measure of uncertainty in the estimate (`std_error` is readily available when you used `collect_metrics(summarize=TRUE)`).
        - Compare estimated test performance across the models. Which models(s) might you prefer?
    d. Use residual plots to evaluate whether some quantitative predictors might be better modeled with nonlinear relationships.
    e. Which variables do you think are the most important predictors of your quantitative outcome? Justify your answer. Do the methods you've applied reach consensus on which variables are most important? What insights are expected? Surprising?
        - Note that if some (but not all) of the indicator terms for a categorical predictor are selected in the final models, the whole predictor should be treated as selected.

<br>

#### Your Work {-}

a & b.

```{r}
# library statements 
# read in data
library(readr)
library(ggplot2)
library(dplyr)
library(tidymodels)
tidymodels_prefer()

fires <- read_csv("forestfires.csv")
```

```{r}
# data cleaning
fires <- fires %>%
    select(-X, -Y, -day) %>%
    filter(month != "nov")
```

```{r}
# creation of cv folds
set.seed(88)
fires_cv10 <- vfold_cv(fires, v = 10)
```

```{r}
# model spec
lm_spec <-
    linear_reg() %>% 
    set_engine(engine = 'lm') %>% 
    set_mode('regression')

lm_lasso_spec <- 
  linear_reg() %>%
  set_args(mixture = 1, penalty = 3.162278) %>%
  set_engine(engine = 'glmnet') %>%
  set_mode('regression') 
```

```{r}
# recipes & workflows
full_rec <- recipe(area ~ ., data = fires) %>%
    step_nzv(all_predictors()) %>% # removes variables with the same value
    step_normalize(all_numeric_predictors()) %>% # important standardization step for LASSO
    step_dummy(all_nominal_predictors())  # creates indicator variables for categorical variables

full_lm_wf <- workflow() %>%
    add_recipe(full_rec) %>%
    add_model(lm_spec)

lasso_wf_tune <- workflow() %>% 
  add_recipe(full_rec) %>%
  add_model(lm_lasso_spec) 
```


```{r}
# fit & tune models
full_lm_wf <- workflow() %>%
    add_recipe(full_rec) %>%
    add_model(lm_spec)
    
full_model <- fit(full_lm_wf, data = fires) 
```

```{r eval=FALSE}
# Tune Model (trying a variety of values of Lambda penalty)
penalty_grid <- grid_regular(
  penalty(range = c(-3, 0.5)), #log10 transformed 
  levels = 30)

tune_output <- tune_grid( # new function for tuning parameters
  lasso_wf_tune, # workflow
  resamples = fires_cv10, # cv folds
  metrics = metric_set(rmse, mae),
  grid = penalty_grid # penalty grid defined above
)
```

```{r}
 autoplot(tune_output) + theme_classic()

 collect_metrics(tune_output) %>%
   filter(.metric == 'rmse') %>% # or choose mae
   select(penalty, rmse = mean)

 best_penalty <- select_best(tune_output, metric = 'rmse')

best_penalty

final_wf <- finalize_workflow(lasso_wf_tune, best_penalty)

final_fit <- fit(final_wf, data = fires)

tidy(final_fit)
```
c.

```{r}
#  calculate/collect CV metrics
full_model %>% tidy()

collect_metrics(tune_output) %>%
  filter(.metric == 'rmse') %>%
  select(penalty, rmse = mean)

best_penalty <- select_by_one_std_err(tune_output, metric = 'rmse', desc(penalty))

final_wf <- finalize_workflow(lasso_wf_tune, best_penalty)

final_fit <- fit(final_wf, data = fires)

tidy(final_fit)
```

 
d.

```{r}
# visual residuals
final_fit %>% tidy() %>% filter(estimate != 0)

lasso_mod_out <- final_fit %>%
    predict(new_data = fires) %>%
    bind_cols(fires) %>%
    mutate(resid = area - .pred)

lm_mod_out <- full_model %>%
    predict(new_data = fires) %>%
    bind_cols(fires) %>%
    mutate(resid = area - .pred)

ggplot(lm_mod_out, aes(x = temp, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic() +
    ylim(-50,150)

ggplot(lasso_mod_out, aes(x = temp, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic() +
    ylim(-50,150)

ggplot(lm_mod_out, aes(x = temp, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic()

ggplot(lasso_mod_out, aes(x = temp, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic()

```

e.

<br>

2. **Summarize investigations**
    - Decide on an overall best model based on your investigations so far. To do this, make clear your analysis goals. Predictive accuracy? Interpretability? A combination of both?
  A mix of interpretability and predictive accuracy is best. The interpradability of our model is important because if someone actually wants to predict a fire using our model, they should only have to take and interpret the measurements that matter the most. The predicive power of the model obviously should not be significantly sacrificed for this goal because predicting fires is the outcome that matters. Taking both of these factors into account, we belive that the LASSO model is far superior to the LM model, because the LASSO model only has 3 relevant predictors and its residual bound is much tighter, especially within the negative residuals, which means that in the LM model there were significantly more false positivies. When false positivies did occur, the LASSO model predicted much smaller fires than the LM model. However, both models were not very good at predicting large fires.


<br>

3. **Societal impact**
    - Are there any harms that may come from your analyses and/or how the data were collected?
    - What cautions do you want to keep in mind when communicating your work?
  If these models are taken to be accurate, there could be real harms that occur. Specifically, that our models do not predict big fires very well at all. That is the take away that should be taken from this analysis, that big fires are very unpredictable.