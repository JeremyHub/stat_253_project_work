```{r hw2_setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, eval=TRUE)
```

# Homework 3 {-}

Jeremy Hubinger, and Tamur Asar


<br><br><br>




## Project Work {-}

### Instructions {-} 

**Goal:** Begin an analysis of your dataset to answer your **regression** research question.

<br>

**Collaboration:** Form a team (2-3 members) for the project and this part can be done as a team. Only one team member should submit a Project Work section. Make sure you include the full names of all of the members in your write up. 

<br>

**Data cleaning:** If your dataset requires any cleaning (e.g., merging datasets, creation of new variables), first consult the [R Resources page](r-resources.html) to see if your questions are answered there. If not, post on the #rcode-questions channel in our Slack workspace to ask for help. *Please ask for help early and regularly* to avoid stressful workloads.

<br>

### Required Analyses {-}

1. **Initial investigation: ignoring nonlinearity (for now)**
    a. Use ordinary least squares (OLS) by using the `lm` engine and LASSO (`glmnet` engine) to build  a series of initial regression models for your quantitative outcome as a function of the predictors of interest. (As part of data cleaning, exclude any variables that you don't want to consider as predictors.)
        - You'll need two model specifications, `lm_spec` and `lm_lasso_spec` (you'll need to tune this one).
    b. For each set of variables, you'll need a `recipe` with the `formula`, `data`, and pre-processing steps
        - You may want to have steps in your recipe that remove variables with near zero variance (`step_nzv()`), remove variables that are highly correlated with other variables (`step_corr()`), normalize all quantitative predictors (`step_normalize(all_numeric_predictors())`) and add indicator variables for any categorical variables (`step_dummy(all_nominal_predictors())`).
        - These models should not include any transformations to deal with nonlinearity. You'll explore this in the next investigation.
    c. Estimate the test performance of the models using CV. Report and interpret (with units) the CV metric estimates along with a measure of uncertainty in the estimate (`std_error` is readily available when you used `collect_metrics(summarize=TRUE)`).
        - Compare estimated test performance across the models. Which models(s) might you prefer?
    d. Use residual plots to evaluate whether some quantitative predictors might be better modeled with nonlinear relationships.
    e. Which variables do you think are the most important predictors of your quantitative outcome? Justify your answer. Do the methods you've applied reach consensus on which variables are most important? What insights are expected? Surprising?
        - Note that if some (but not all) of the indicator terms for a categorical predictor are selected in the final models, the whole predictor should be treated as selected.

<br>

#### Your Work {-}

a & b.

```{r}
# library statements 
# read in data
library(readr)
library(ggplot2)
library(dplyr)
library(tidymodels)
library(rpart.plot)
library(vip)
tidymodels_prefer()

fires_raw <- read_csv("forestfires.csv")
```

The dataset has a variety of variables that might impact the strength and likelihood of fires. These variables include mainly things about weather, like temperature, pressure, wind, rain, ect. The variable we are trying to predict is "area" which is the area burned by fires on that day.

```{r}
# data cleaning
fires <- fires_raw %>%
    # we get rid of day because there isnt a huge reason that it should impact the fires, we are more interested in how weather factors impact fires, not day of the week
    select(-day) %>%
   filter(area > 0) %>%
    mutate(area = log(area+.1))
```

```{r}
# creation of cv folds
set.seed(88)
fires_cv10 <- vfold_cv(fires, v = 10)
```

We now fit multiple types of models. One model is fit using simple LM regression with all predictors. The second model is fit using LASSO regression to attempt to provide some amount of filtering to the model predictors. Both models are fit and compared later on.

```{r}
# model spec
lm_spec <-
    linear_reg() %>% 
    set_engine(engine = 'lm') %>% 
    set_mode('regression')

# this is the LASSO model
lm_lasso_spec <- 
  linear_reg() %>%
  set_args(mixture = 1, penalty = tune()) %>%
  set_engine(engine = 'glmnet') %>%
  set_mode('regression')
```

```{r}
# recipes & workflows
# both models are fit with the same recipe because they both start with all predictors and all predictors are normalized
full_rec <- recipe(area ~ ., data = fires) %>%
    step_nzv(all_predictors()) %>% # removes variables with the same value
    step_normalize(all_numeric_predictors()) %>% # important standardization step for LASSO
    step_dummy(all_nominal_predictors())  # creates indicator variables for categorical variables

full_lm_wf <- workflow() %>%
    add_recipe(full_rec) %>%
    add_model(lm_spec)

lasso_wf_tune <- workflow() %>% 
  add_recipe(full_rec) %>%
  add_model(lm_lasso_spec) 
```

```{r}
# fit & tune models
full_lm_wf <- workflow() %>%
    add_recipe(full_rec) %>%
    add_model(lm_spec)
    
full_model <- fit(full_lm_wf, data = fires) 
```

```{r}
# Tune Model (trying a variety of values of Lambda penalty)
penalty_grid <- grid_regular(
  penalty(range = c(-5, 3)), #log10 transformed 
  levels = 30)

tune_output <- tune_grid( # new function for tuning parameters
  lasso_wf_tune, # workflow
  resamples = fires_cv10, # cv folds
  metrics = metric_set(rmse, mae),
  grid = penalty_grid # penalty grid defined above
)
```

```{r}
 autoplot(tune_output) + theme_classic()

 collect_metrics(tune_output) %>%
   filter(.metric == 'rmse') %>% # or choose mae
   select(penalty, rmse = mean)

 best_penalty <- select_best(tune_output, metric = 'rmse')

best_penalty

final_wf <- finalize_workflow(lasso_wf_tune, best_penalty)

final_fit <- fit(final_wf, data = fires)

tidy(final_fit)
```
It seems as though all variables were set to 0 with LASSO. This is very interesting because it indicates that none of the variables make a significant effect in the predictive power of the model.

c.

```{r}
#  calculate/collect CV metrics

full_model %>% tidy()

best_penalty <- select_best(tune_output, metric = 'rmse')

final_wf <- finalize_workflow(lasso_wf_tune, best_penalty)

final_fit <- fit(final_wf, data = fires)

tidy(final_fit)
```
Based on the above information, using the LASSO model to tell what variables are the most important (which is none). This is much more readable than the LM model which has a lot of importance on a lot of different variables which don't seem to matter, and actually hurt the model's performance on test data.

```{r}
LASSO_model_cv <- fit_resamples(final_fit,
  resamples = fires_cv10, 
  metrics = metric_set(rmse, rsq, mae)
)

LASSO_model_cv %>% collect_metrics(summarize=TRUE)

LM_model_cv <- fit_resamples(full_model,
  resamples = fires_cv10, 
  metrics = metric_set(rmse, rsq, mae)
)

LM_model_cv %>% collect_metrics(summarize=TRUE)
```
As we can see in the metrics above, the normal LM model with all the variables performs worse than the LASSO model which is much simpler. Because of the better performance (likely due to overfitting in the LM model's case) and simpler model coefficients we prefer the LASSO model at this point, but better models are yet to come.
 
d.

```{r}
# visual residuals
final_fit %>% tidy() %>% filter(estimate != 0)

lasso_mod_out <- final_fit %>%
    predict(new_data = fires) %>%
    bind_cols(fires) %>%
    mutate(resid = area - .pred)

lm_mod_out <- full_model %>%
    predict(new_data = fires) %>%
    bind_cols(fires) %>%
    mutate(resid = area - .pred)

ggplot(lm_mod_out, aes(x = temp, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic() +
    ggtitle("LM model residual plot")

ggplot(lasso_mod_out, aes(x = temp, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic() +
    ggtitle("LASSO model residual plot")

```

e.

As mentioned earlier, the variables don't seem to be impactful on the performance of the model, Which is interesting because it means that the predictors are fairly useless when trying to predict fires, this just goes to show the unpredictive nature of wildfires. We can also see that in the residual plots above, given that the LM model and the LASSO model (which has all coefficients set to 0) because the residual plots don't look very different.

<br>  

Part 2:

```{r}
# Natural Spline Recipe
ns2_rec <- full_rec %>%
  step_ns(temp, deg_free = 3) %>%
  step_ns(RH, deg_free = 3) %>%
  step_ns(DMC, deg_free = 3) %>%
  step_ns(FFMC, deg_free = 3) %>%
  step_ns(DC, deg_free = 3) %>%
  step_ns(ISI, deg_free = 3) %>%
  step_ns(wind, deg_free = 3) # natural cubic spline (higher deg_free means more knots)

# Workflow (Recipe + Model)
wf <- workflow() %>%
    add_recipe(ns2_rec) %>%
    add_model(lm_spec)

# CV to Evaluate
cv_output <- fit_resamples(
  wf, # workflow
  resamples = fires_cv10, # cv folds
  metrics = metric_set(rsq, mae, rmse)
)
```

```{r}
# Fit with all data
ns_mod <- fit(
  wf, #workflow
  data = fires
)

ns_mod_output <- fires %>%
  bind_cols(predict(ns_mod, new_data = fires)) %>%
    mutate(resid = area - .pred)

ggplot(ns_mod_output, aes(y=resid,x=temp)) +
    geom_point() +
    geom_smooth(color = "blue", se = FALSE) +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic()

ggplot(ns_mod_output, aes(y=resid,x=DMC)) +
    geom_point() +
    geom_smooth(color = "blue", se = FALSE) +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic()

ggplot(ns_mod_output, aes(y=resid,x=RH)) +
    geom_point() +
    geom_smooth(color = "blue", se = FALSE) +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic()

ggplot(ns_mod_output, aes(y=resid,x=wind)) +
    geom_point() +
    geom_smooth(color = "blue", se = FALSE) +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic()
```

```{r}
cv_output %>% collect_metrics(summarize = TRUE) # splines
LM_model_cv %>% collect_metrics(summarize=TRUE)
LASSO_model_cv %>% collect_metrics(summarize=TRUE)
```
Above we can see that the splines did not improve our model that much. This can be largely explained by the lack of non-linearity in our predictors (as we can see in the plots above), as well as the general overall un-importance of our predictors (as we can see the LASSO model with all coefficients at 0 still performs the best).


GAM WITH SPLINES
```{r}
gam_spec <- 
  gen_additive_mod() %>%
  set_engine(engine = 'mgcv') %>%
  set_mode('regression') 

gam_mod <- fit(gam_spec,
    area ~ s(DMC) + s(temp) + s(RH) + s(wind) + s(ISI) + s(DC) + s(FFMC),
    data = fires
)
```

```{r}
par(mfrow=c(2,2))
gam_mod %>% pluck('fit') %>% mgcv::gam.check() 
```

```{r}
gam_mod %>% pluck('fit') %>% summary() 
```
In the above table you can see that the r-squared is 0.2 and that the model only explains 5.3% of the variance in the outcome. This is quite bad (especially when compared to our other models) and again comes down to the lack of non-linearity in the data (which can also be observed below) and the overall lack of variable importance for any variable in the dataset.

```{r}
gam_mod %>% pluck('fit') %>% plot( all.terms = TRUE, pages = 1)
```

From the plots we can see that most of our data is linear (hovering around 1 edf with the default range) while RH is the most nonlinear plot (with an edf of 2). As GAM regression is primarily to generate models explaining nonlinearity in multiple predictors, GAMs are most likely not the ideal model to be using in order to perform predictions on the data.

<br>

HW 4 CLASSIFICATION:
```{r}
fires_category <- fires_raw %>%
  select(-day) %>%
  mutate(area = case_when(area>0~1,area==0~0)) %>%
  mutate(area = as.factor(area)) #%>%
  #mutate(season = case_when(month=="dec"||month=="jan"||month=="feb"~"winter",
                            #month=="mar"||month=="apr"||month=="may"~"spring",
                            #month=="june"||month=="jul"||month=="aug"~"summer",
                            #month=="sep"||month=="oct"||month=="nov"~"fall"))
```

LOGISTIC REGRESSION:
```{r}
fires_category <- fires_category %>%
  mutate(area = relevel(factor(area), ref='0')) # set ref level

# log reg mode spec
logistic_spec <- logistic_reg() %>%
    set_engine('glm') %>%
    set_mode('classification')

# recipe
logistic_rec <- recipe(area ~ temp + RH + DMC + X + DC + Y + FFMC, data = fires_category)

# workflow: rec + model
log_wf <- workflow() %>% 
    add_recipe(logistic_rec) %>%
    add_model(logistic_spec) 

# fit
log_fit <- fit(log_wf, data = fires_category)
```

```{r}
log_fit %>% tidy()
```
```{r}
log_fit %>% tidy() %>%
  mutate(OR.conf.low = exp(estimate - 1.96*std.error), OR.conf.high = exp(estimate + 1.96*std.error)) %>% # do this first
  mutate(OR = exp(estimate))
```
Larger the OR value the higher the chance there is a fire.
Originally, we included month which showed that the categorical variable December month indicates a high chance of a fire while January and November indicate a very low chance of a fire. This was because of a lack of data: there were 9 "true" cases for fires in December and 0 "false" cases. This caused the  standard errors of these months to be very high, indicating that month is a bad predictor for logistic regression despite being an effective predictor with the trees method. In the current adata we remove month from our model.

This data shows that temp, RH, DMC, X, Y, DC, and FFMC are all relatively similar at predicting whether there was a fire or not. 

TREES and BAGGING:
```{r}
set.seed(123) # don't change this

data_fold <- vfold_cv(fires_category, v = 10)

ct_spec_tune <- decision_tree() %>%
  set_engine(engine = 'rpart') %>%
  set_args(cost_complexity = tune(),  
           min_n = 2, 
           tree_depth = NULL) %>% 
  set_mode('classification') 

data_rec <- recipe(area ~ ., data = fires_category)

data_wf_tune <- workflow() %>%
  add_model(ct_spec_tune) %>%
  add_recipe(data_rec)

param_grid <- grid_regular(cost_complexity(range = c(-5, 2)), levels = 10) 

tune_res <- tune_grid(
  data_wf_tune, 
  resamples = data_fold, 
  grid = param_grid, 
  metrics = metric_set(accuracy) #change this for regression trees
)
```

```{r}
autoplot(tune_res) + theme_classic()
```

```{r}
best_complexity <- select_by_one_std_err(tune_res, metric = 'accuracy', desc(cost_complexity))
data_wf_final <- finalize_workflow(data_wf_tune, best_complexity)

land_final_fit <- fit(data_wf_final, data = fires_category)


tune_res %>% 
  collect_metrics() %>%
  filter(cost_complexity == best_complexity%>%pull(cost_complexity))
```

```{r}
par(mfrow = c(1,3))
land_final_fit %>% extract_fit_engine() %>% rpart.plot()
```
```{r}
land_final_fit %>%
  extract_fit_engine() %>%
  pluck('variable.importance')
```
```{r}
# Model Specification
rf_spec <- rand_forest() %>%
  set_engine(engine = 'ranger') %>% 
  set_args(mtry = NULL, # size of random subset of variables; default is floor(sqrt(number of total predictors))
           trees = 1000, # Number of trees
           min_n = 2,
           probability = FALSE, # FALSE: get hard predictions (not needed for regression)
           importance = 'impurity') %>% # we'll come back to this at the end
  set_mode('classification') # change this for regression

# Recipe
data_rec <- recipe(area ~ ., data = fires_category)
```


```{r}
# Workflows
data_wf_mtry2 <- workflow() %>%
  add_model(rf_spec %>% set_args(mtry = 2)) %>%
  add_recipe(data_rec)

data_wf_mtry10 <- workflow() %>%
  add_model(rf_spec %>% set_args(mtry = 10)) %>%
  add_recipe(data_rec)

## Create workflows for mtry = 12, 74, and 147

data_wf_mtry12 <- workflow() %>%
  add_model(rf_spec %>% set_args(mtry = 12)) %>%
  add_recipe(data_rec)

data_wf_mtry6 <- workflow() %>%
  add_model(rf_spec %>% set_args(mtry = 6)) %>%
  add_recipe(data_rec)
```

```{r}
# Fit Models
set.seed(123) # make sure to run this before each fit so that you have the same 1000 trees
data_fit_mtry2 <- fit(data_wf_mtry2, data = fires_category)

# Fit models for 12
set.seed(123) 
data_fit_mtry12 <- fit(data_wf_mtry12, data = fires_category)

set.seed(123) 
data_fit_mtry10 <- fit(data_wf_mtry10, data = fires_category)

set.seed(123) 
data_fit_mtry6 <- fit(data_wf_mtry6, data = fires_category)
```

```{r}
# Custom Function to get OOB predictions, true observed outcomes and add a user-provided model label
rf_OOB_output <- function(fit_model, model_label, truth){
    tibble(
          .pred_class = fit_model %>% extract_fit_engine() %>% pluck('predictions'), #OOB predictions
          area = truth,
          label = model_label
      )
}
```

```{r}
# Evaluate OOB Metrics

data_rf_OOB_output <- bind_rows(
    rf_OOB_output(data_fit_mtry2,2, fires_category %>% pull(area)),
    rf_OOB_output(data_fit_mtry12,12, fires_category %>% pull(area)),
    rf_OOB_output(data_fit_mtry10,10, fires_category %>% pull(area)),
    rf_OOB_output(data_fit_mtry6,6, fires_category %>% pull(area))
)

data_rf_OOB_output = data_rf_OOB_output %>% 
    group_by(label) %>%
    accuracy(truth = area, estimate = .pred_class)
```

```{r}
ggplot(data=data_rf_OOB_output, aes(y=.estimate,x=label)) +
    geom_point() +
    theme_classic()
```
```{r}
data_fit_mtry12
```
```{r}
rf_OOB_output(data_fit_mtry12,12, fires_category %>% pull(area)) %>%
    conf_mat(truth = area, estimate= .pred_class)
```
```{r}
model_output2 <- data_wf_mtry12 %>% 
  update_model(rf_spec %>% set_args(importance = "permutation")) %>% #based on permutation
  fit(data = fires_category) %>% 
    extract_fit_engine() 

model_output2 %>% 
    vip(num_features = 30) + theme_classic()


model_output2 %>% vip::vi() %>% head()
model_output2 %>% vip::vi() %>% tail()
```

2. **Summarize investigations**
    - Decide on an overall best model based on your investigations so far. To do this, make clear your analysis goals. Predictive accuracy? Interpretability? A combination of both?
  A mix of interpretability and predictive accuracy is best. The interpradability of our model is important because if someone actually wants to predict a fire using our model, they should only have to take and interpret the measurements that matter the most. The predictive power of the model obviously should not be significantly sacrificed for this goal because predicting fires is the outcome that matters.
  Taking these factors into account, we do not belive that any of our models are best. That is to say that none of our models have sufficient predictive power to be used to predict fires. We can see this in the LASSO model and specifically the fact that all of the coefficients in the model were set to 0, indicating that none of them are important. We can also see this through the error metrics for our models. The LASSO model had the best error metrics when preforming corss validation, which means that all the other models (splines, normal LM, and our GAM models) were all overfit to training data.


<br>

3. **Societal impact**
    - Are there any harms that may come from your analyses and/or how the data were collected?
    - What cautions do you want to keep in mind when communicating your work?
  If these models are taken to be accurate, there could be real harms that occur. Specifically, that our models do not predict fires very well at all. That is the take away that should be taken from this analysis, that fires are very unpredictable. This is actually a very important take-away and something that would certainly be useful for fire experts to know.
  Another possible harm is if the area variable is interpreted as pure area, without consideration for the transformation that occurred prior to analysis. The area was transformed with an ln(x+1) function. Therefore, all predictions should be interpreted through the inverse of this transform to be interpreted in any meaningful way. That being said, the predictions from the models should not be interpreted at all, because, as mentioned above, all predictinos are quite bad.











