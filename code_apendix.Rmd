```{r code_apendix, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, warning = FALSE, message = FALSE, tidy = TRUE)
```

### Code Apendix {-}

Jeremy Hubinger, and Tamur Asar

```{r}
# library statements 
# read in data
library(readr)
library(ggplot2)
library(dplyr)
library(tidymodels)
library(rpart.plot)
library(vip)
tidymodels_prefer()

fires_raw <- read_csv("forestfires.csv")
```

```{r}
# data cleaning
fires <- fires_raw %>%
    # we get rid of day because there isnt a huge reason that it should impact the fires, we are more interested in how weather factors impact fires, not day of the week
    select(-day) %>%
   filter(area > 0) %>%
    mutate(area = log(area+.1)) %>%
  mutate(X = factor(X), Y = factor(Y)) %>%
  mutate(geoGrids = factor(paste0(X,'-',Y))) %>%
  mutate(season = case_when(month=="dec"|month=="jan"|month=="feb"~"winter",
                            month=="mar"|month=="apr"|month=="may"~"spring",
                            month=="jun"|month=="jul"|month=="aug"~"summer",
                            month=="sep"|month=="oct"|month=="nov"~"fall"))
```


## EDA

```{r}
fires %>%
  count(geoGrids)

fires %>%
  count(X,Y) %>%
  ggplot(aes(x = X, y = Y,size = n)) + geom_point()


fires %>%
  count(X,Y,season) %>%
  ggplot(aes(x = X, y = Y,size = n)) + geom_point() + facet_wrap(~season)


fires %>%
  ggplot(aes(x = area)) + geom_density() + facet_grid(Y~X)


fires %>%
  ggplot(aes(x = temp, y = area, color = factor(cut(ISI,4)))) + geom_point() + geom_smooth(se = FALSE)

fires %>%
  ggplot(aes(x = temp, y = area, color = season)) + geom_point() + geom_smooth(se = FALSE)



```
##LM and LASSO models

```{r}
# creation of cv folds
set.seed(88)
fires_cv10 <- vfold_cv(fires, v = 10)
```

```{r}
# model spec
lm_spec <-
    linear_reg() %>% 
    set_engine(engine = 'lm') %>% 
    set_mode('regression')

# this is the LASSO model
lm_lasso_spec <- 
  linear_reg() %>%
  set_args(mixture = 1, penalty = tune()) %>%
  set_engine(engine = 'glmnet') %>%
  set_mode('regression')
```

```{r}
# recipes & workflows
# both models are fit with the same recipe because they both start with all predictors and all predictors are normalized
full_rec <- recipe(area ~ ., data = fires) %>%
  step_rm(month,X,Y,rain,wind,temp,RH) %>%
    step_nzv(all_predictors()) %>% # removes variables with the same value
    #step_novel(all_nominal_predictors()) %>%
    step_normalize(all_numeric_predictors()) %>% # important standardization step for LASSO
    step_dummy(all_nominal_predictors())  # creates indicator variables for categorical variables

full_lm_wf <- workflow() %>%
    add_recipe(full_rec) %>%
    add_model(lm_spec)

lasso_wf_tune <- workflow() %>% 
  add_recipe(full_rec) %>%
  add_model(lm_lasso_spec) 
```

```{r}
# fit & tune models
full_lm_wf <- workflow() %>%
    add_recipe(full_rec) %>%
    add_model(lm_spec)
    
full_model <- fit(full_lm_wf, data = fires) 
```

```{r}
# Tune Model (trying a variety of values of Lambda penalty)
penalty_grid <- grid_regular(
  penalty(range = c(-5, 3)), #log10 transformed 
  levels = 30)

tune_output <- tune_grid( # new function for tuning parameters
  lasso_wf_tune, # workflow
  resamples = fires_cv10, # cv folds
  metrics = metric_set(rmse, mae),
  grid = penalty_grid # penalty grid defined above
)
```

```{r}
 autoplot(tune_output) + theme_classic()

 collect_metrics(tune_output) %>%
   filter(.metric == 'rmse') %>% # or choose mae
   select(penalty, rmse = mean)

 best_penalty <- select_best(tune_output, metric = 'rmse')

best_penalty

final_wf <- finalize_workflow(lasso_wf_tune, best_penalty)

final_fit <- fit(final_wf, data = fires)

tidy(final_fit)
```

```{r}
#  calculate/collect CV metrics

full_model %>% tidy()

best_penalty <- select_best(tune_output, metric = 'rmse')

final_wf <- finalize_workflow(lasso_wf_tune, best_penalty)

final_fit <- fit(final_wf, data = fires)

tidy(final_fit)

#Lasso Var Importance
glmnet_output <- final_fit %>% extract_fit_engine()
    
# Create a boolean matrix (predictors x lambdas) of variable exclusion
bool_predictor_exclude <- glmnet_output$beta==0

# Loop over each variable
var_imp <- sapply(seq_len(nrow(bool_predictor_exclude)), function(row) {
    this_coeff_path <- bool_predictor_exclude[row,]
    if(sum(this_coeff_path) == ncol(bool_predictor_exclude)){ return(0)}else{
    return(ncol(bool_predictor_exclude) - which.min(this_coeff_path) + 1)}
})

# Create a dataset of this information and sort
var_imp_data <- tibble(
    var_name = rownames(bool_predictor_exclude),
    var_imp = var_imp
)
var_imp_data %>% arrange(desc(var_imp))
```

```{r}
tune_output %>% collect_metrics() %>% filter(penalty == (best_penalty %>% pull(penalty)))

LM_model_cv <- fit_resamples(full_lm_wf,
  resamples = fires_cv10, 
  metrics = metric_set(rmse, rsq, mae)
)

LM_model_cv %>% collect_metrics(summarize=TRUE)
```

```{r}
# visual residuals
final_fit %>% tidy() %>% filter(estimate != 0)

lasso_mod_out <- final_fit %>%
    predict(new_data = fires) %>%
    bind_cols(fires) %>%
    mutate(resid = area - .pred)

lm_mod_out <- full_model %>%
    predict(new_data = fires) %>%
    bind_cols(fires) %>%
    mutate(resid = area - .pred)

ggplot(lm_mod_out, aes(x = .pred, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic() +
    ggtitle("LM model residual plot")

ggplot(lasso_mod_out, aes(x = .pred, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic() +
    ggtitle("LASSO model residual plot")

```

```{r}
# Natural Spline Recipe
ns2_rec <- full_rec %>%
  #step_ns(temp, deg_free = 3) %>%
  #step_ns(RH, deg_free = 3) %>%
  step_ns(DMC, deg_free = 3) %>%
  step_ns(FFMC, deg_free = 3) %>%
  step_ns(DC, deg_free = 3) %>%
  step_ns(ISI, deg_free = 3) #%>%
  #step_ns(wind, deg_free = 3) # natural cubic spline (higher deg_free means more knots)

# Workflow (Recipe + Model)
wf <- workflow() %>%
    add_recipe(ns2_rec) %>%
    add_model(lm_spec)


# CV to Evaluate
cv_output <- fit_resamples(
  wf, # workflow
  resamples = fires_cv10, # cv folds
  metrics = metric_set(rsq, mae, rmse)
)
```

```{r}
# Fit with all data
ns_mod <- fit(
  wf, #workflow
  data = fires
)

ns_mod_output <- fires %>%
  bind_cols(predict(ns_mod, new_data = fires)) %>%
    mutate(resid = area - .pred)

ggplot(ns_mod_output, aes(y=resid,x=.pred)) +
    geom_point() +
    geom_smooth(color = "blue", se = FALSE) +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic()

ggplot(ns_mod_output, aes(y=resid,x=.pred)) +
    geom_point() +
    geom_smooth(color = "blue", se = FALSE) +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic()

ggplot(ns_mod_output, aes(y=resid,x=DMC)) +
    geom_point() +
    geom_smooth(color = "blue", se = FALSE) +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic()

ggplot(ns_mod_output, aes(y=resid,x=ISI)) +
    geom_point() +
    geom_smooth(color = "blue", se = FALSE) +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic()

ggplot(ns_mod_output, aes(y=resid,x=wind)) +
    geom_point() +
    geom_smooth(color = "blue", se = FALSE) +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic()
```

```{r}
cv_output %>% collect_metrics(summarize = TRUE) # splines
LM_model_cv %>% collect_metrics(summarize=TRUE)
tune_output %>% collect_metrics(summarize=TRUE) %>% filter(penalty == (best_penalty %>% pull(penalty)))
```

#GAM WITH SPLINES
```{r}
gam_spec <- 
  gen_additive_mod() %>%
  set_engine(engine = 'mgcv') %>%
  set_mode('regression') 

gam_mod <- fit(gam_spec,
    area ~ s(DMC) + s(temp) + s(RH) + s(wind) + s(ISI) + s(DC) + s(FFMC),
    data = fires
)
```

```{r}
par(mfrow=c(2,2))
gam_mod %>% pluck('fit') %>% mgcv::gam.check() 
```

```{r}
gam_mod %>% pluck('fit') %>% summary() 
```

```{r}
gam_mod %>% pluck('fit') %>% plot( all.terms = TRUE, pages = 1)
```
#Classification setup
```{r}
fires_category <- fires_raw %>%
  select(-day) %>%
  mutate(area = case_when(area>0~1,area==0~0)) %>%
  mutate(area = as.factor(area)) %>%
  mutate(X = factor(X), Y = factor(Y)) %>%
  mutate(geoGrids = factor(paste0(X,'-',Y))) %>%
  mutate(season = case_when(month=="dec"|month=="jan"|month=="feb"~"winter",
                            month=="mar"|month=="apr"|month=="may"~"spring",
                            month=="jun"|month=="jul"|month=="aug"~"summer",
                            month=="sep"|month=="oct"|month=="nov"~"fall"))
```

#LOGISTIC REGRESSION:
```{r}
fires_category <- fires_category %>%
  mutate(area = relevel(factor(area), ref='0')) # set ref level

# log reg mode spec
logistic_spec <- logistic_reg() %>%
    set_engine('glm') %>%
    set_mode('classification')

# recipe
logistic_rec <- recipe(area ~ ., data = fires_category) %>%
    step_rm(month,X,Y,rain,wind,temp,RH) %>%
    step_nzv(all_predictors()) %>% # removes variables with the same value
    #step_novel(all_nominal_predictors()) %>%
    #step_normalize(all_numeric_predictors()) %>% # important standardization step for LASSO
    step_dummy(all_nominal_predictors())  # creates indicator variables for categorical variables

# workflow: rec + model
log_wf <- workflow() %>% 
    add_recipe(logistic_rec) %>%
    add_model(logistic_spec) 

# fit
log_fit <- fit(log_wf, data = fires_category)
```

```{r}
log_fit %>% tidy()
```
```{r}
log_fit %>% tidy() %>%
  mutate(OR.conf.low = exp(estimate - 1.96*std.error), OR.conf.high = exp(estimate + 1.96*std.error)) %>% # do this first
  mutate(OR = exp(estimate))
```
#TREES and BAGGING:
```{r}
set.seed(123) # don't change this

data_fold <- vfold_cv(fires_category, v = 10)

ct_spec_tune <- decision_tree() %>%
  set_engine(engine = 'rpart') %>%
  set_args(cost_complexity = tune(),  
           min_n = 2, 
           tree_depth = NULL) %>% 
  set_mode('classification') 

data_rec <- recipe(area ~ ., data = fires_category)

data_wf_tune <- workflow() %>%
  add_model(ct_spec_tune) %>%
  add_recipe(data_rec)

param_grid <- grid_regular(cost_complexity(range = c(-5, 2)), levels = 20) 

tune_res <- tune_grid(
  data_wf_tune, 
  resamples = data_fold, 
  grid = param_grid, 
  metrics = metric_set(accuracy) #change this for regression trees
)
```

```{r}
autoplot(tune_res) + theme_classic()
```

```{r}
best_complexity <- select_best(tune_res, metric = 'accuracy', desc(cost_complexity))
data_wf_final <- finalize_workflow(data_wf_tune, best_complexity)

land_final_fit <- fit(data_wf_final, data = fires_category)


tune_res %>% 
  collect_metrics() %>%
  filter(cost_complexity == best_complexity%>%pull(cost_complexity))
```

```{r}

land_final_fit %>% extract_fit_engine() %>% rpart.plot()
```
```{r}
land_final_fit %>%
  extract_fit_engine() %>%
  pluck('variable.importance')
```
```{r}
# Model Specification
rf_spec <- rand_forest() %>%
  set_engine(engine = 'ranger') %>% 
  set_args(mtry = NULL, # size of random subset of variables; default is floor(sqrt(number of total predictors))
           trees = 1000, # Number of trees
           min_n = 2,
           probability = FALSE, # FALSE: get hard predictions (not needed for regression)
           importance = 'impurity') %>% # we'll come back to this at the end
  set_mode('classification') # change this for regression

# Recipe
data_rec <- recipe(area ~ ., data = fires_category)
```

```{r}
# Workflows
data_wf_rf <- workflow() %>%
  add_model(rf_spec) %>%
  add_recipe(data_rec)
```

```{r}
# Fit Models
set.seed(123) # make sure to run this before each fit so that you have the same 1000 trees
data_fit_rf <- fit(data_wf_rf , data = fires_category)


```

```{r}
# Evaluate OOB Metrics

data_rf_OOB_output <- tibble(
          .pred_class = data_fit_rf %>% extract_fit_engine() %>% pluck('predictions'), #OOB predictions
          area = fires_category %>% pull(area)
      )

data_rf_OOB_output %>% 
    accuracy(truth = area, estimate = .pred_class)
```


```{r}
data_fit_rf
```
```{r}
data_rf_OOB_output %>%
    conf_mat(truth = area, estimate= .pred_class)
```
```{r}
model_output2 <- data_wf_rf %>% 
  update_model(rf_spec %>% set_args(importance = "permutation")) %>% #based on permutation
  fit(data = fires_category) %>% 
    extract_fit_engine() 

model_output2 %>% 
    vip(num_features = 30) + theme_classic()


model_output2 %>% vip::vi() %>% head()
model_output2 %>% vip::vi() %>% tail()
```

#CLUSTERING

```{r}
fires_sub <- fires %>%
    select(-month,-X,-Y,-rain,-wind,-temp,-RH, -season, -geoGrids)

dist_mat_scaled <- dist(scale(fires_sub))

summary(fires_sub)

hc_complete <- hclust(dist_mat_scaled, method = "complete")

plot(hc_complete)
```

```{r}
fires <- fires %>%
    mutate(
        hclust_height = factor(cutree(hc_complete, h = 4.5)), # Cut at height (h) 3
        hclust_num6 = factor(cutree(hc_complete, k = 6)), # Cut into 6 clusters (k)
        temp_factor = if(temp>25) 25 else 0,
        temp_factor = if(temp_factor==0) (if(temp>10) 10 else 0) else temp_factor,
        temp_factor = if(temp_factor==0) 0 else temp_factor
    )

ggplot(fires, aes(x = hclust_height, y = area)) +
    geom_boxplot() +
    facet_wrap(vars(factor(temp_factor))) +
    theme_classic()

ggplot(fires, aes(x = hclust_height, y = DMC)) +
    geom_boxplot() +
    facet_wrap(vars(factor(temp_factor))) +
    theme_classic()

ggplot(fires, aes(x = hclust_height, y = temp)) +
    geom_boxplot() +
    facet_wrap(vars(factor(temp_factor))) +
    theme_classic()

ggplot(fires, aes(x = hclust_height, y = geoGrids)) +
    geom_boxplot() +
    facet_wrap(vars(factor(temp_factor))) +
    theme_classic()
```
