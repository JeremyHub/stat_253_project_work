---
title: "AsarHubingerClassification"
output:
  html_document: default
  pdf_document: default
---

```{r hw2_setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, eval=TRUE)
```

# Homework 4 {-}

Jeremy Hubinger, and Tamur Asar

```{r}
# library statements 
# read in data
library(readr)
library(ggplot2)
library(dplyr)
library(tidymodels)
library(rpart.plot)
library(vip)
tidymodels_prefer()

fires_raw <- read_csv("forestfires.csv")
```

```{r}
# data cleaning
fires <- fires_raw %>%
    # we get rid of day because there isnt a huge reason that it should impact the fires, we are more interested in how weather factors impact fires, not day of the week
    select(-day) %>%
   filter(area > 0) %>%
    mutate(area = log(area+.1)) %>% 
  # transform month to season for larger groupings
  mutate(season = case_when(month=="dec"|month=="jan"|month=="feb"~"winter",
                            month=="mar"|month=="apr"|month=="may"~"spring",
                            month=="june"|month=="jul"|month=="aug"~"summer",
                            month=="sep"|month=="oct"|month=="nov"~"fall"))

```

```{r}
fires_category <- fires_raw %>%
  mutate(area = case_when(area>0~1,area==0~0)) %>%
  mutate(area = as.factor(area)) %>%
  # transform month to season for larger groupings
  mutate(season = case_when(month=="dec"|month=="jan"|month=="feb"~"winter",
                            month=="mar"|month=="apr"|month=="may"~"spring",
                            month=="june"|month=="jul"|month=="aug"~"summer",
                            month=="sep"|month=="oct"|month=="nov"~"fall"))
```

RESEARCH QUESTION:
Which predictors are best at determining whether a fire occurred or not?

CLASSIFICATION METHODS:

LOGISTIC REGRESSION:
```{r}
fires_category <- fires_category %>%
  mutate(area = relevel(factor(area), ref='0')) # set ref level

# log reg mode spec
logistic_spec <- logistic_reg() %>%
    set_engine('glm') %>%
    set_mode('classification')

# recipe
logistic_rec <- recipe(area ~ temp + RH + DMC + X + DC + Y + FFMC + season, data = fires_category)

# workflow: rec + model
log_wf <- workflow() %>% 
    add_recipe(logistic_rec) %>%
    add_model(logistic_spec) 

# fit
log_fit <- fit(log_wf, data = fires_category)
```

```{r}
log_fit %>% tidy()
```

```{r}
log_fit %>% tidy() %>%
  mutate(OR.conf.low = exp(estimate - 1.96*std.error), OR.conf.high = exp(estimate + 1.96*std.error)) %>% # do this first
  mutate(OR = exp(estimate))
```
The larger the OR value the higher the chance there is a fire.
Originally, we included month which showed that the categorical variable December month indicates a high chance of a fire while January and November indicate a very low chance of a fire. This was because of a lack of data: there were 9 "true" cases for fires in December and 0 "false" cases. This caused the  standard errors of these months to be very high, indicating that month is a bad predictor for logistic regression despite being an effective predictor with the trees method.

In the current data we remove month from our model and instead include season which aggregates the months by their respective seasons in Portugal (where the data set is taken from). This creates broader groupings that circumvents our previous issue of having insufficient data to make predictions. This data shows that temp, RH, DMC, X, Y, DC, and FFMC are all relatively similar at predicting whether there was a fire or not.
As far as seasons go, fall (the default) is the least indicative of a fire while spring and summer are similar to the other predictors. However, winter appears to have the strongest correlation with there being a fire. This is interesting as winter is classified as December, January, and February. As established above, December in the data has many fires while January has next to none.

TREES and BAGGING:
```{r}
set.seed(123) # don't change this

data_fold <- vfold_cv(fires_category, v = 10)

ct_spec_tune <- decision_tree() %>%
  set_engine(engine = 'rpart') %>%
  set_args(cost_complexity = tune(),  
           min_n = 2, 
           tree_depth = NULL) %>% 
  set_mode('classification') 

data_rec <- recipe(area ~ ., data = fires_category)

data_wf_tune <- workflow() %>%
  add_model(ct_spec_tune) %>%
  add_recipe(data_rec)

param_grid <- grid_regular(cost_complexity(range = c(-5, 1)), levels = 10) 

tune_res <- tune_grid(
  data_wf_tune, 
  resamples = data_fold, 
  grid = param_grid, 
  metrics = metric_set(accuracy) #change this for regression trees
)
```

```{r}
autoplot(tune_res) + theme_classic()
```

```{r}
best_complexity <- select_by_one_std_err(tune_res, metric = 'accuracy', desc(cost_complexity))
data_wf_final <- finalize_workflow(data_wf_tune, best_complexity)

land_final_fit <- fit(data_wf_final, data = fires_category)


tune_res %>% 
  collect_metrics() %>%
  filter(cost_complexity == best_complexity%>%pull(cost_complexity))
```

```{r}
par(mfrow = c(1,3))
land_final_fit %>% extract_fit_engine() %>% rpart.plot()
```
```{r}
land_final_fit %>%
  extract_fit_engine() %>%
  pluck('variable.importance')
```

CLUSTERING

```{r}
fires_sub <- fires %>%
    select(temp, RH, DMC, X, DC, Y, FFMC)

dist_mat_scaled <- dist(scale(fires_sub))

summary(fires_sub)

hc_complete <- hclust(dist_mat_scaled, method = "complete")

plot(hc_complete)
```

```{r}
fires <- fires %>%
    mutate(
        hclust_height = factor(cutree(hc_complete, h = 5)), # Cut at height (h) 3
        hclust_num6 = factor(cutree(hc_complete, k = 6)), # Cut into 6 clusters (k)
        temp_factor = if(temp>25) 25 else 0,
        temp_factor = if(temp_factor==0) (if(temp>10) 10 else 0) else temp_factor,
        temp_factor = if(temp_factor==0) 0 else temp_factor
    )

ggplot(fires, aes(x = hclust_height, y = area)) +
    geom_boxplot() +
    facet_wrap(vars(factor(temp_factor))) +
    theme_classic()
```

This model shows us that clustering methods are not effective on our dataset as there is no strong correlation between any key variables and the measured area of the fire. If in the plot above, we saw that there are some clusters that only have small fires, and some clusters that only have large fires, that would tell us that clustering would be effective at predicting fires. However, that is not what we see. We can see that the area of the fires within each cluster are quite spread out, meaining that clusters isn't a very good way of predicting fires based on our data.

we tried k means but liked dendrograms more
```{r eval=FALSE}
fires_sub <- fires %>%
    select(temp, RH, DMC, X, DC, Y, FFMC)

# Data-specific function to cluster and calculate total within-cluster SS
fires_cluster_ss <- function(k){
    # Perform clustering
    kclust <- kmeans(scale(fires_sub), centers = k)

    # Return the total within-cluster sum of squares
    return(kclust$tot.withinss)
}

tibble(
    k = 1:15,
    tot_wc_ss = purrr::map_dbl(1:15, fires_cluster_ss)
) %>% 
    ggplot(aes(x = k, y = tot_wc_ss)) +
    geom_point() + 
    labs(x = "Number of clusters",y = 'Total within-cluster sum of squares') + 
    theme_classic()
    
```

```{r eval=FALSE}
kclust_k3 <- kmeans(fires_sub, centers = 3)

# Display the cluter assignments
kclust_k3$cluster

# Add a variable (kclust_3) to the original dataset 
# containing the cluster assignments
fires <- fires %>%
    mutate(kclust_3 = factor(kclust_k3$cluster))

ggplot(fires, aes(x=X, y=Y, colour=kclust_3, size = area)) +
    geom_point()
```